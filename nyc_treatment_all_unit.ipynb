{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7127694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Path to the folder where the files are located and where the results will be saved\n",
    "main_data_path = r\"C:pythonprojetcs_Dani\\nyc_taxi_data\"\n",
    "output_folder = r\"C:nyc_taxi_cleaned\"\n",
    "lookup_table_path = r\"taxi_zone_lookup.csv\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "try:\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output directory created: {output_folder}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating output directory: {e}\")\n",
    "\n",
    "# Load the lookup table\n",
    "try:\n",
    "    Zone_table = pd.read_csv(lookup_table_path, encoding='utf-8')\n",
    "    print(\"Lookup file successfully loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the lookup file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Display basic information for initial validation\n",
    "print(\"\\nLookup Table Info:\")\n",
    "print(Zone_table.info())\n",
    "\n",
    "# Mapping of payment types and vendors\n",
    "payment_type_mapping = {\n",
    "    1: 'Credit Card',\n",
    "    2: 'Cash',\n",
    "    3: 'No Charge',\n",
    "    4: 'Dispute',\n",
    "    0: 'Unknown'\n",
    "}\n",
    "vendor_mapping = {\n",
    "    1: 'Creative Mobile Technologies (CMT)',\n",
    "    2: 'VeriFone Inc.'\n",
    "}\n",
    "\n",
    "# Function to process data from a single file\n",
    "def process_data(file_path, file_name):\n",
    "    try:\n",
    "        print(f\"Processing file data: {file_path}...\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Remove null values in essential columns\n",
    "        df_data = df.dropna(subset=['total_amount', 'passenger_count', 'PULocationID', 'DOLocationID'])\n",
    "\n",
    "        # Select only necessary columns for analysis\n",
    "        columns_to_keep = [\n",
    "            'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count',\n",
    "            'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount',\n",
    "            'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
    "            'total_amount', 'payment_type', 'VendorID'\n",
    "        ]\n",
    "        df_data = df_data[columns_to_keep]\n",
    "\n",
    "        # Convert datetime columns\n",
    "        df_data['tpep_pickup_datetime'] = pd.to_datetime(df_data['tpep_pickup_datetime'], errors='coerce')\n",
    "        df_data['tpep_dropoff_datetime'] = pd.to_datetime(df_data['tpep_dropoff_datetime'], errors='coerce')\n",
    "\n",
    "        # Create columns for duration and datetime info\n",
    "        df_data['trip_duration_minutes'] = (df_data['tpep_dropoff_datetime'] - df_data['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "        # Adjust mappings\n",
    "        df_data['payment_type_description'] = df_data['payment_type'].map(payment_type_mapping)\n",
    "        df_data['vendor_name_description'] = df_data['VendorID'].map(vendor_mapping)\n",
    "\n",
    "        # Add real names for Pickup and Dropoff locations\n",
    "        df_data = df_data.merge(\n",
    "            Zone_table,\n",
    "            left_on='PULocationID',\n",
    "            right_on='LocationID',\n",
    "            how='left'\n",
    "        ).rename(columns={\"Zone\": \"Pickup_Location\", \"Borough\": \"Pickup_Borough\"})\n",
    "        df_data.drop(columns=['LocationID'], inplace=True)\n",
    "\n",
    "        df_data = df_data.merge(\n",
    "            Zone_table,\n",
    "            left_on='DOLocationID',\n",
    "            right_on='LocationID',\n",
    "            how='left'\n",
    "        ).rename(columns={\"Zone\": \"Dropoff_Location\", \"Borough\": \"Dropoff_Borough\"})\n",
    "        df_data.drop(columns=['LocationID'], inplace=True)\n",
    "\n",
    "        # Remove duplicates and unnecessary columns\n",
    "        df_data = df_data.drop_duplicates()\n",
    "        df_data.drop(columns=['PULocationID', 'DOLocationID', 'payment_type', 'VendorID'], inplace=True)\n",
    "\n",
    "        # Select 100,000 random rows\n",
    "        if len(df_data) > 100000:\n",
    "            df_data = df_data.sample(n=100000, random_state=42)\n",
    "\n",
    "        # Ensure the output file path is correct\n",
    "        output_file = os.path.join(output_folder, f\"{file_name.replace('.parquet', '_cleaned.csv')}\")\n",
    "\n",
    "        # Save the cleaned data\n",
    "        df_data.to_csv(output_file, index=False)\n",
    "        print(f\"Cleaned data from file {file_name} successfully saved!\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "# Loop to process all .parquet files inside the main folder\n",
    "for file_name in os.listdir(main_data_path):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(main_data_path, file_name)\n",
    "        try:\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            process_data(file_path, file_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "print(\"Processing completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
